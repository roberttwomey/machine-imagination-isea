{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_to_image_BigGAN_CLIP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/machine-imagination-workshop/blob/main/text_to_image_BigGAN_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# BigGAN + CLIP + CMA-ES\n",
        "\n",
        "This notebook explores the new methods of text-to-image synthesis that are made possible with the release of the CLIP model from OpenAI.\n",
        "\n",
        "You will provide an input phrase (as text), and the networks here will search for an approximate generated image representation of that phrase, according to CLIP's understanding of images as text.\n",
        "\n",
        "We will follow the steps below. For each cell, click the \"play\" button at the left to run the code within. I've tried to include approximate run times for each step, so you have a sense of how long you will be waiting at each point. \n",
        "\n",
        "Please stop me with any questions you have, and chime in with insights!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKlThat3lOa1"
      },
      "source": [
        "Credits: This notebook is based off of [j.mp/wanderclip](https://j.mp/wanderclip) by Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com). I've modified it to store noise/class vectors for reuse, and additionally adapted it to run on [nautilus.optiputer.net](https://nautilus.optiputer.net)/z8 (not relevant here, because we're on colab because it's free)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmKTmvBg7z5",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Setup software libraries (run once)\n",
        "#@markdown This cell installs the software libraries necessary to run our \n",
        "#@markdown text-to-image code on this Colab instance: CUDA, torch, torchvision.\n",
        "\n",
        "#@markdown Run this cell once - press the play button at top left.\n",
        "\n",
        "#@markdown (this takes about 4-5 minutes to run)\n",
        "\n",
        "#@markdown Afterwards, restart the kernel. Select __Runtime -> Restart runtime__\n",
        "#@markdown from the top menu. Move on to Step 2 once you have restarted.\n",
        "\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "# prints out what graphics card we have\n",
        "!nvidia-smi -L\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SDpkkK7cU1y",
        "cellView": "form"
      },
      "source": [
        "#@title 2. Install ML Models\n",
        "#@markdown Installs BigGAN, CLIP, and CMA â€” the image generator network, image \n",
        "#@markdown description network, and optimizer/search function respectively. \n",
        "\n",
        "#@markdown (this takes around 1 minute to run)\n",
        "\n",
        "# BigGAN\n",
        "!pip install pytorch-pretrained-biggan\n",
        "from pytorch_pretrained_biggan import BigGAN\n",
        "last_gen_model = 'biggan-deep-512'\n",
        "biggan_model = BigGAN.from_pretrained(last_gen_model).cuda().eval()\n",
        "\n",
        "# CLIP (from github)\n",
        "%cd /content\n",
        "!git clone --depth 1 https://github.com/openai/CLIP\n",
        "!pip install ftfy\n",
        "%cd /content/CLIP\n",
        "\n",
        "# load CLIP\n",
        "import clip\n",
        "last_clip_model = 'ViT-B/32'\n",
        "perceptor, preprocess = clip.load(last_clip_model)\n",
        "\n",
        "# Natural Language Tool Kit (NLTK)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# CMA\n",
        "!pip install cma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWzPLrBbdxW",
        "cellView": "form"
      },
      "source": [
        "#@title 3. Generate an Image for your text!\n",
        "#@markdown This cell takes a textual input and generates (searches for) a \n",
        "#@markdown corresponding image, as CLIP sees things. Replace the \"prompt\" below \n",
        "#@markdown with the string you wish you explore. You may also want to experiment\n",
        "#@markdown with the random seed, and with the number of iterations, as described\n",
        "#@markdown below. \n",
        "\n",
        "#@markdown When the process is complete, the browser will download your result\n",
        "#@markdown (an image) as well as a video of the training process, and a noise \n",
        "#@markdown vector and class vector, the two numbers we need to recreate your results at\n",
        "#@markdown a later date. \n",
        "\n",
        "#@markdown (Note: on a Tesla T4, each text-to-image run takes about 12 minutes \n",
        "#@markdown at ~7.3 seconds/iteration, with 100 iterations to arrive at the \n",
        "#@markdown \"final\" image)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Prompt\n",
        "#@markdown For the prompt, OpenAI suggests to use the template \"A photo of a X.\" or \"A photo of a X, a type of Y.\"\n",
        "#@markdown (see the [paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf))\n",
        "#@markdown But you should experiment.\n",
        "# #@markdown 2. For **initial_class** you can either use free text or select a special option from the drop-down list. (I suggest leaving this unchanged)\n",
        "# #@markdown 3. Free text and 'From prompt' might fail to find an appropriate ImageNet class.\n",
        "# #@markdown 4. **seed**=0 means no seed.\n",
        "prompt = 'a photo of a sunrise through a window' #@param {type:'string'}\n",
        "\n",
        "#@markdown ### Parameters to vary results\n",
        "#@markdown changing the seed will produce different results for the same prompt\n",
        "#@markdown (put in a different integer):\n",
        "seed =  1#@param {type:'number'}\n",
        "\n",
        "#@markdown changing the iterations will change how long it spends convering on a \n",
        "#@markdown a result. Longer times will hone in on a given representation,\n",
        "#@markdown determined by the prompt and random seed\n",
        "iterations =  50#@param {type:'integer'}\n",
        "terminal_iterations =  50#@param {type:'integer'}\n",
        "\n",
        "\n",
        "#@markdown ### Parameters you likely won't want to change\n",
        "gen_model = 'biggan-deep' #@param ['biggan-deep', 'sigmoid']\n",
        "size = '512' #@param [512, 256, 128] \n",
        "color = True #@param {type:'boolean'}\n",
        "initial_class = 'Random mix' #@param ['From prompt', 'Random class', 'Random Dirichlet', 'Random mix', 'Random embeddings'] {allow-input: true}\n",
        "optimize_class = True #@param {type:'boolean'}\n",
        "class_smoothing = 0.1 #@param {type:'number'}\n",
        "truncation = 1 #@param {type:'number'}\n",
        "stochastic_truncation = False #@param {type:'boolean'}\n",
        "optimizer = 'CMA-ES' #@param ['SGD','Adam','CMA-ES','CMA-ES + SGD interleaved','CMA-ES + Adam interleaved','CMA-ES + terminal SGD','CMA-ES + terminal Adam']\n",
        "pop_size = 50 #@param {type:'integer'}\n",
        "clip_model = 'ViT-B/32' #@param ['ViT-B/32','RN50','RN101','RN50x4']\n",
        "augmentations =  64#@param {type:'integer'}\n",
        "learning_rate =  0.1#@param {type:'number'}\n",
        "noise_normality_loss =  0#@param {type:'number'}\n",
        "embed_normality_loss = 0 #@param {type:'number'}\n",
        "minimum_entropy_loss = 0.0001 #@param {type:'number'}\n",
        "total_variation_loss = 0.1 #@param {type:'number'}\n",
        "show_every = 1 #@param {type:'integer'}\n",
        "save_every = 1 #@param {type:'integer'}\n",
        "fps = 2 #@param {type:'number'}\n",
        "freeze_secs = 0 #@param {type:'number'}\n",
        "\n",
        "if seed == 0:\n",
        "  seed = None\n",
        "\n",
        "softmax_temp = 1\n",
        "emb_factor = 0.067 #calculated empirically \n",
        "loss_factor = 100\n",
        "sigma0 = 0.5 #http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#practical\n",
        "cma_adapt = True\n",
        "cma_diag = 'sigmoid' in gen_model\n",
        "cma_active = True\n",
        "cma_elitist = False\n",
        "noise_size = 128\n",
        "class_size = 128 if initial_class.lower()=='random embeddings' else 1000\n",
        "channels = 3 if color else 1\n",
        "\n",
        "gen_model = gen_model + '-' + size\n",
        "if gen_model != last_gen_model and 'biggan' in gen_model:\n",
        "  biggan_model = BigGAN.from_pretrained(gen_model).cuda().eval()\n",
        "  last_gen_model = gen_model\n",
        "if clip_model != last_clip_model:\n",
        "  perceptor, preprocess = clip.load(clip_model)\n",
        "  last_clip_model = clip_model\n",
        "clip_res = perceptor.input_resolution.item()\n",
        "sideX = sideY = int(size)\n",
        "if sideX<=clip_res and sideY<=clip_res:\n",
        "  augmentations = 1\n",
        "if 'CMA' not in optimizer:\n",
        "  pop_size = 1\n",
        "if 'biggan' not in gen_model:\n",
        "  optimize_class = False\n",
        "requires_grad = ('SGD' in optimizer or 'Adam' in optimizer) and ('terminal' not in optimizer or terminal_iterations>0)\n",
        "total_iterations = iterations + terminal_iterations*('terminal' in optimizer)\n",
        "\n",
        "def my_forward(self, z, class_label, truncation):\n",
        "  assert 0 < truncation <= 1\n",
        "\n",
        "  if initial_class.lower()=='random embeddings':\n",
        "    embed = class_label\n",
        "  else:\n",
        "    embed = self.embeddings(class_label)\n",
        "    \n",
        "  cond_vector = torch.cat((z, embed), dim=1)\n",
        "\n",
        "  z = self.generator(cond_vector, truncation)\n",
        "  return z\n",
        "BigGAN.forward = my_forward\n",
        "\n",
        "!rm -rf /content/output\n",
        "!mkdir -p /content/output\n",
        "\n",
        "import numpy as np\n",
        "state = None if not seed else np.random.RandomState(seed)\n",
        "np.random.seed(seed)\n",
        "import torch\n",
        "import torchvision\n",
        "import sys\n",
        "torch.manual_seed(np.random.randint(sys.maxsize))\n",
        "import imageio\n",
        "from IPython.display import HTML, Image, clear_output\n",
        "from scipy.stats import truncnorm, dirichlet\n",
        "from pytorch_pretrained_biggan import BigGAN, convert_to_images, one_hot_from_names, utils\n",
        "from torch import nn\n",
        "from nltk.corpus import wordnet as wn\n",
        "from base64 import b64encode\n",
        "from time import time\n",
        "import cma\n",
        "from cma.sigma_adaptation import CMAAdaptSigmaCSA, CMAAdaptSigmaTPA\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)\n",
        "\n",
        "def replace_to_inplace_relu(model): #saves memory; from https://github.com/minyoungg/pix2latent/blob/master/pix2latent/model/biggan.py\n",
        "    for child_name, child in model.named_children():\n",
        "        if isinstance(child, nn.ReLU):\n",
        "            setattr(model, child_name, nn.ReLU(inplace=False))\n",
        "        else:\n",
        "            replace_to_inplace_relu(child)\n",
        "    return\n",
        "replace_to_inplace_relu(biggan_model)\n",
        "replace_to_inplace_relu(perceptor)\n",
        "\n",
        "ind2name = {index: wn.of2ss('%08dn'%offset).lemma_names()[0] for offset, index in utils.IMAGENET.items()}\n",
        "\n",
        "def save(out,name=None):\n",
        "  with torch.no_grad():\n",
        "    out = out.cpu().numpy()\n",
        "  img = convert_to_images(out)[0]\n",
        "  if name:\n",
        "    imageio.imwrite(name, np.asarray(img))\n",
        "  return img\n",
        "\n",
        "# twomey additions\n",
        "def save_vec(out,name):\n",
        "  with torch.no_grad():\n",
        "    vec = out.cpu().numpy()\n",
        "  np.savetxt(name, vec)\n",
        "\n",
        "# twomey additions\n",
        "def save_all_vecs(out, name):\n",
        "  with torch.no_grad():\n",
        "    vec = out.cpu().numpy()\n",
        "  np.savetxt(name, vec)\n",
        "\n",
        "hist = []\n",
        "def checkin(i, best_ind, total_losses, losses, regs, out, noise=None, emb=None, probs=None):\n",
        "  global sample_num, hist\n",
        "  name = None\n",
        "  if save_every and i%save_every==0:\n",
        "    name = '/content/output/frame_%05d.jpg'%sample_num\n",
        "  pil_image = save(out, name)\n",
        "  vals0 = [sample_num, i, total_losses[best_ind], losses[best_ind], regs[best_ind], np.mean(total_losses), np.mean(losses), np.mean(regs), np.std(total_losses), np.std(losses), np.std(regs)]\n",
        "  stats = 'sample=%d iter=%d best: total=%.2f cos=%.2f reg=%.3f avg: total=%.2f cos=%.2f reg=%.3f std: total=%.2f cos=%.2f reg=%.3f'%tuple(vals0)\n",
        "  vals1 = []\n",
        "  if noise is not None:\n",
        "    vals1 = [np.mean(noise), np.std(noise)]\n",
        "    stats += ' noise: avg=%.2f std=%.3f'%tuple(vals1)\n",
        "  vals2 = []\n",
        "  if emb is not None:\n",
        "    vals2 = [emb.mean(),emb.std()]\n",
        "    stats += ' emb: avg=%.2f std=%.3f'%tuple(vals2)\n",
        "  elif probs:\n",
        "    best = probs[best_ind]\n",
        "    inds = np.argsort(best)[::-1]\n",
        "    probs = np.array(probs)\n",
        "    vals2 = [ind2name[inds[0]], best[inds[0]], ind2name[inds[1]], best[inds[1]], ind2name[inds[2]], best[inds[2]], np.sum(probs >= 0.5)/pop_size,np.sum(probs >= 0.3)/pop_size,np.sum(probs >= 0.1)/pop_size]\n",
        "    stats += ' 1st=%s(%.2f) 2nd=%s(%.2f) 3rd=%s(%.2f) components: >=0.5:%.0f, >=0.3:%.0f, >=0.1:%.0f'%tuple(vals2)\n",
        "  hist.append(vals0+vals1+vals2)\n",
        "  if show_every and i%show_every==0:\n",
        "    clear_output()\n",
        "    display(pil_image)  \n",
        "  print(stats)\n",
        "  print('Best index: %s' % best_ind)\n",
        "  sample_num += 1\n",
        "\n",
        "eps = 1e-8\n",
        "class_vector = None\n",
        "if 'sigmoid' in gen_model:\n",
        "  noise_size = channels*sideY*sideX\n",
        "  noise_vector = np.random.rand(pop_size, noise_size).astype(np.float32)\n",
        "  noise_vector = np.log((noise_vector+eps)/(1-noise_vector+eps))\n",
        "else:\n",
        "  noise_vector = truncnorm.rvs(-2*truncation, 2*truncation, size=(pop_size, noise_size), random_state=state).astype(np.float32) #see https://github.com/tensorflow/hub/issues/214\n",
        "\n",
        "  if initial_class.lower() == 'random class':\n",
        "    class_vector = np.ones(shape=(pop_size, class_size), dtype=np.float32)*class_smoothing/999\n",
        "    class_vector[0,np.random.randint(class_size)] = 1-class_smoothing\n",
        "  elif initial_class.lower() == 'random dirichlet':\n",
        "    class_vector = dirichlet.rvs([pop_size/class_size] * class_size, size=1, random_state=state).astype(np.float32)\n",
        "  elif initial_class.lower() == 'random mix':\n",
        "    class_vector = np.random.rand(pop_size, class_size).astype(np.float32)\n",
        "  elif initial_class.lower() == 'random embeddings':\n",
        "    class_vector = np.random.randn(pop_size, class_size).astype(np.float32)\n",
        "  else:\n",
        "    if initial_class.lower() == 'from prompt':\n",
        "      initial_class = prompt\n",
        "    try:\n",
        "      class_vector = None\n",
        "      class_vector = one_hot_from_names(initial_class, batch_size=pop_size)\n",
        "      assert class_vector is not None\n",
        "      class_vector = class_vector*(1-class_smoothing*class_size/(class_size-1))+class_smoothing/(class_size-1)\n",
        "    except Exception as e:  \n",
        "      print('Error: could not find initial_class. Try something else.')\n",
        "      raise e\n",
        "\n",
        "  if initial_class.lower() != 'random embeddings':\n",
        "    class_vector = class_vector/np.sum(class_vector,axis=-1, keepdims=True)\n",
        "    class_vector = np.log(class_vector+eps)-np.mean(np.log(class_vector+eps),axis=-1, keepdims=True)\n",
        "  initial_class_vector = class_vector[0]\n",
        "  if initial_class.lower() in ('random mix','random embeddings'):\n",
        "    initial_class_vector = initial_class_vector*0\n",
        "  class_vector = torch.tensor(class_vector, requires_grad=requires_grad, device='cuda')\n",
        "  smoothed_ent = -torch.tensor(class_smoothing*np.log(class_smoothing/999+eps)+(1-class_smoothing)*np.log(1-class_smoothing+eps), dtype=torch.float32).cuda()\n",
        "noise_vector = torch.tensor(noise_vector, requires_grad=requires_grad, device='cuda')\n",
        "\n",
        "if requires_grad:\n",
        "  params = [noise_vector]\n",
        "  if optimize_class:\n",
        "    params = params + [class_vector]\n",
        "  if 'SGD' in optimizer:\n",
        "    optim = torch.optim.SGD(params, lr=learning_rate, momentum=0.9)  \n",
        "  else:\n",
        "    optim = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "tx = clip.tokenize(prompt)\n",
        "with torch.no_grad():\n",
        "  target_clip = perceptor.encode_text(tx.cuda())\n",
        "\n",
        "def get_output(noise_vector, class_vector):\n",
        "  save_class_vector_norm = None\n",
        "  if 'sigmoid' in gen_model:\n",
        "    out = noise_vector.sigmoid().reshape(1, channels, sideY, sideX)*2-1\n",
        "  else:\n",
        "    if stochastic_truncation: #https://arxiv.org/abs/1702.04782\n",
        "      with torch.no_grad():\n",
        "        trunc_indices = noise_vector.abs() > 2*truncation\n",
        "        size = torch.count_nonzero(trunc_indices).cpu().numpy()\n",
        "        trunc = truncnorm.rvs(-2*truncation, 2*truncation, size=(1,size)).astype(np.float32)\n",
        "        noise_vector.data[trunc_indices] = torch.tensor(trunc, requires_grad=requires_grad, device='cuda')\n",
        "    else:\n",
        "      noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
        "    if initial_class.lower() == 'random embeddings':\n",
        "      class_vector_norm = class_vector*emb_factor\n",
        "    else:\n",
        "      class_vector_norm = torch.softmax(class_vector/softmax_temp,dim=-1)\n",
        "    out = biggan_model(noise_vector, class_vector_norm, truncation)\n",
        "    if channels==1:\n",
        "      out = out.mean(dim=1, keepdim=True)\n",
        "    if initial_class.lower() != 'random embeddings':\n",
        "      save_class_vector_norm = class_vector_norm\n",
        "  if channels==1:\n",
        "    out = out.repeat(1,3,1,1)\n",
        "  return out, save_class_vector_norm\n",
        "\n",
        "def normality_loss(vec): #https://arxiv.org/abs/1903.00925\n",
        "    mu2 = vec.mean().square()\n",
        "    sigma2 = vec.var()\n",
        "    return mu2+sigma2-torch.log(sigma2)-1\n",
        "\n",
        "# twomey\n",
        "def make_safe_filename(s):\n",
        "    def safe_char(c):\n",
        "        if c.isalnum():\n",
        "            return c\n",
        "        else:\n",
        "            return \"_\"\n",
        "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")\n",
        "\n",
        "global_best_loss = np.inf\n",
        "global_best_iteration = 0\n",
        "global_best_noise_vector = None\n",
        "global_best_class_vector = None\n",
        "\n",
        "def ascend_txt(i, grad_step=False, show_save=False):\n",
        "  global global_best_loss, global_best_iteration, global_best_noise_vector, global_best_class_vector\n",
        "  prev_class_vector_norms = []\n",
        "  regs = []\n",
        "  losses = []\n",
        "  total_losses = []\n",
        "  best_loss = np.inf\n",
        "  global_reg = torch.tensor(0, device='cuda', dtype=torch.float32, requires_grad=grad_step)\n",
        "  if 'biggan' in gen_model:\n",
        "    if optimize_class and embed_normality_loss and initial_class.lower() == 'random embeddings':\n",
        "      global_reg = global_reg+embed_normality_loss*normality_loss(class_vector)\n",
        "    if noise_normality_loss:\n",
        "      global_reg = global_reg+noise_normality_loss*normality_loss(noise_vector)\n",
        "    global_reg = loss_factor*global_reg  \n",
        "    if grad_step:\n",
        "      global_reg.backward()\n",
        "  for j in range(pop_size):\n",
        "    p_s = []\n",
        "    out, class_vector_norm = get_output(noise_vector[j:j+1], None if class_vector is None else class_vector[j:j+1])\n",
        "    if class_vector_norm is not None:\n",
        "      with torch.no_grad():\n",
        "        prev_class_vector_norms.append(class_vector_norm.cpu().numpy()[0])\n",
        "    \n",
        "    for aug in range(augmentations):\n",
        "      if sideX<=clip_res and sideY<=clip_res or augmentations==1:\n",
        "        apper = out  \n",
        "      else:\n",
        "        size = torch.randint(int(.7*sideX), int(.98*sideX), ())\n",
        "        offsetx = torch.randint(0, sideX - size, ())\n",
        "        offsety = torch.randint(0, sideX - size, ())\n",
        "        apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "      apper = (apper+1)/2\n",
        "      apper = nn.functional.interpolate(apper, clip_res, mode='bilinear')\n",
        "      #apper = apper.clamp(0,1)\n",
        "      p_s.append(apper)\n",
        "    into = nom(torch.cat(p_s, 0))\n",
        "    predict_clip = perceptor.encode_image(into)\n",
        "    loss = loss_factor*(1-torch.cosine_similarity(predict_clip, target_clip).mean())\n",
        "    total_loss = loss\n",
        "    regs.append(global_reg.item())\n",
        "    if 'sigmoid' in gen_model and total_variation_loss or 'biggan' in gen_model and optimize_class and minimum_entropy_loss and initial_class.lower() != 'random embeddings':\n",
        "      if 'sigmoid' in gen_model and total_variation_loss:\n",
        "        reg = total_variation_loss*((out[:, :, :-1, :] - out[:, :, 1:, :]).abs().mean() + (out[:, :, :, :-1] - out[:, :, :, 1:]).abs().mean())\n",
        "      elif 'biggan' in gen_model and optimize_class and minimum_entropy_loss and initial_class.lower() != 'random embeddings':\n",
        "        reg = minimum_entropy_loss*((-class_vector_norm*torch.log(class_vector_norm+eps)).sum()-smoothed_ent).abs()\n",
        "      reg = loss_factor*reg\n",
        "      total_loss = total_loss + reg\n",
        "      with torch.no_grad():\n",
        "        regs[-1] += reg.item()\n",
        "    with torch.no_grad():\n",
        "      losses.append(loss.item())\n",
        "      total_losses.append(total_loss.item()+global_reg.item())\n",
        "    if total_losses[-1]<best_loss:\n",
        "      best_loss = total_losses[-1]\n",
        "      best_ind = j\n",
        "      best_out = out\n",
        "      if best_loss < global_best_loss:\n",
        "        global_best_loss = best_loss\n",
        "        global_best_iteration = i\n",
        "        with torch.no_grad():\n",
        "          global_best_noise_vector = noise_vector[best_ind]\n",
        "          if class_vector is not None:\n",
        "            global_best_class_vector = class_vector[best_ind]\n",
        "\n",
        "    if grad_step:    \n",
        "      total_loss.backward()\n",
        "\n",
        "  if grad_step:\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "  if show_save and (save_every and i % save_every == 0 or show_every and i % show_every == 0):\n",
        "    noise = None\n",
        "    emb = None\n",
        "    if 'biggan' in gen_model:\n",
        "      with torch.no_grad():\n",
        "        noise = noise_vector.cpu().numpy()\n",
        "        if initial_class.lower() == 'random embeddings':\n",
        "          emb = class_vector.cpu().numpy()\n",
        "    checkin(i, best_ind, total_losses, losses, regs, best_out, noise, emb, prev_class_vector_norms)  \n",
        "  return total_losses, best_ind\n",
        "\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "if 'CMA' in optimizer:\n",
        "  initial_vector = np.zeros(noise_size)\n",
        "  bounds = None\n",
        "  #if 'biggan' in gen_model and not stochastic_truncation:\n",
        "  #  bounds = [-2*truncation*np.ones(noise_size),2*truncation*np.ones(noise_size)]\n",
        "  if optimize_class:\n",
        "    initial_vector = np.hstack([initial_vector, initial_class_vector])\n",
        "    #if not stochastic_truncation:\n",
        "    #  bounds[0] = list(bounds[0]) + [None]*class_size\n",
        "    #  bounds[1] = list(bounds[1]) + [None]*class_size\n",
        "  cma_opts = {'popsize': pop_size, 'seed': np.nan, 'AdaptSigma': cma_adapt, 'CMA_diagonal': cma_diag, 'CMA_active': cma_active, 'CMA_elitist':cma_elitist, 'bounds':bounds}\n",
        "  cmaes = cma.CMAEvolutionStrategy(initial_vector, sigma0, inopts=cma_opts)\n",
        "\n",
        "sample_num = 0\n",
        "machine = !nvidia-smi -L\n",
        "start = time()\n",
        "for i in range(total_iterations):    \n",
        "  if 'CMA' in optimizer and i<iterations:\n",
        "    with torch.no_grad():\n",
        "      cma_results = torch.tensor(cmaes.ask(), dtype=torch.float32).cuda()\n",
        "      if optimize_class:\n",
        "        noise_vector.data, class_vector.data = torch.split_with_sizes(cma_results, (noise_size, class_size), dim=-1)\n",
        "        class_vector.data = class_vector.data\n",
        "      else:\n",
        "        noise_vector.data = cma_results      \n",
        "  if requires_grad and ('terminal' not in optimizer or i>=iterations):\n",
        "    losses, best_ind = ascend_txt(i, grad_step=True, show_save='CMA' not in optimizer or i>=iterations)\n",
        "    assert noise_vector.requires_grad and noise_vector.is_leaf and (not optimize_class or class_vector.requires_grad and class_vector.is_leaf), (noise_vector.requires_grad, noise_vector.is_leaf, class_vector.requires_grad, class_vector.is_leaf)\n",
        "  if 'CMA' in optimizer and i<iterations:\n",
        "    with torch.no_grad():\n",
        "      losses, best_ind = ascend_txt(i, show_save=True)\n",
        "      if i<iterations-1:\n",
        "        if optimize_class:\n",
        "          vectors = torch.cat([noise_vector,class_vector], dim=1)\n",
        "        else:\n",
        "          vectors = noise_vector\n",
        "        cmaes.tell(vectors.cpu().numpy(), losses)\n",
        "      elif 'terminal' in optimizer and terminal_iterations:\n",
        "        pop_size = 1\n",
        "        noise_vector[0] = global_best_noise_vector\n",
        "        if class_vector is not None:\n",
        "          class_vector[0] = global_best_class_vector\n",
        "  if save_every and i % save_every == 0 or show_every and i % show_every == 0:\n",
        "    print('took: %d secs (%.2f sec/iter) on %s. CUDA memory: %.1f GB'%(time()-start,(time()-start)/(i+1), machine[0], torch.cuda.max_memory_allocated()/1024**3))\n",
        "\n",
        "out, _ = get_output(global_best_noise_vector.unsqueeze(0), None if global_best_class_vector is None else global_best_class_vector.unsqueeze(0))\n",
        "name = '/content/%s_%d.jpg'%(prompt, seed)\n",
        "pil_image = save(out,name)  \n",
        "save_vec(global_best_noise_vector, '/content/%s_%d_noise.txt' % (prompt, seed))\n",
        "save_vec(global_best_class_vector, '/content/%s_%d_class.txt' % (prompt, seed))\n",
        "display(pil_image)  \n",
        "print('best_loss=%.2f best_iter=%d'%(global_best_loss,global_best_iteration))\n",
        "\n",
        "from google.colab import files, output\n",
        "files.download('/content/%s_%d.jpg'%(prompt, seed))\n",
        "files.download('/content/%s_%d_noise.txt' % (prompt, seed))\n",
        "files.download('/content/%s_%d_class.txt' % (prompt, seed))\n",
        "\n",
        "out = '\"/content/%s_%d.mp4\"'%(prompt, seed)\n",
        "with open('/content/list.txt','w') as f:\n",
        "  for i in range(sample_num):\n",
        "    f.write('file /content/output/frame_%05d.jpg\\n'%i)\n",
        "  for j in range(int(freeze_secs*fps)):\n",
        "    f.write('file /content/output/frame_%05d.jpg\\n'%i)\n",
        "!ffmpeg -r $fps -f concat -safe 0 -i /content/list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
        "with open('/content/%s_%d.mp4'%(prompt, seed), 'rb') as f:\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "display(HTML(\"\"\"\n",
        "  <video controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\"\"\" % data_url))\n",
        "\n",
        "from google.colab import files, output\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "files.download('/content/%s_%d.mp4'%(prompt, seed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5YjaXywie10"
      },
      "source": [
        "# Explanation\n",
        "\n",
        "There are three parts to this generative approach:\n",
        "1. BigGAN is our image generation network\n",
        "2. CLIP is our text-to-image association network (textual descriptions, really)\n",
        "3. CMA-ES is our search/optimizer strategy.\n",
        "\n",
        "### BigGAN\n",
        "\n",
        "BigGAN (https://arxiv.org/abs/1809.11096) set a standard for high resolution, high fidelity image synthesis in 2018. It contained four times as many parameters and eight times the batch size of previous models, and synthesized a state of the art 512 x 512 pixel image across [1000 different classes](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt) from [Imagenet](https://www.image-net.org/). It was also prohibitively expensive to train! Thankfully Google/Google Brain has released a number of pretrained models for us to explore.\n",
        "\n",
        "BigGAN takes two inputs: a 256-dimensional \"noise\" vector, and a 1000-dimensional one hot \"class\" vector. The \"class\" selects which category of image it is trying to generate (or what mix of categories). The noise vector (latent vector) determines the appearance of this particular instance from within the category (\"dog\" from \"dogs\").\n",
        "\n",
        "You can use a different generative network with CLIP to achieve a similar aim. For instance, people are experimenting with StyleGAN2, [StyleGAN2 ADA](https://colab.research.google.com/drive/1J8xyNRTNVnkNbQJnidcgSdDCHHKfGa8N?usp=sharing#scrollTo=I-YJmx89HLro), [DALL-Es encoder](https://colab.research.google.com/drive/1NGM9L8qP0gwl5z5GAuB_bd0wTNsxqclG), [Lucent's FFT](https://colab.research.google.com/github/eps696/aphantasia/blob/master/Illustra.ipynb). Each generative approach will have different qualities, aesthetics, representational range due to training data and method.\n",
        "\n",
        "### CLIP\n",
        "OpenAI's CLIP CLIP (Contrastive Language-Image Pre-Training) is the key part of this text-to-image translation. It was trained on a massive dataset of unfiltered, highly varied, and highly noisy data: textâ€“image pairs from the internet. See details on the [CLIP Model Card](https://github.com/openai/CLIP/blob/main/model-card.md#data). It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task. Learn more at [github.com/openai/CLIP](https://github.com/openai/CLIP), including the [Interacting with Clip colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).\n",
        "\n",
        "\n",
        "### CMA-ES\n",
        "\n",
        "Covariance matrix adaptation evolution strategy (CMA-ES) is a strategy for numerical optimization. (from [wikipedia](https://en.wikipedia.org/wiki/CMA-ES))\n",
        "\n",
        "This is our strategy for searching what combinations of noise + class vector (BigGAN inputs) produce the best representation of the prompt, according to CLIP (which knows how to relate images and textual descriptions). From a given starting point (random class, random noise), CMA-ES guides the changes in class and noise to improve the output image from BigGAN, to better satisfy CLIP. \n",
        "\n",
        "The \"evolutionary\" aspect of CMA-ES describes how it produces a population of candidate values (in our case 50, set by **`pop_size`** above), and selects the most successful (according to cosine distance with CLIP) to produce the next round of candidates. Repeating this over 50 iterations, we select the best out of all 2500 vectors evaluated..\n",
        "\n",
        "You can use this same GAN + CLIP architecture with a different optimizer to achieve a similar aim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq9sxuzjgBWX"
      },
      "source": [
        "# Activities\n",
        "- Try experimenting with different prompts, but leave the other fields the same.\n",
        "  - Change the prompt and select \"Runtime->restart and run all\" from the top menu. \n",
        "- Try textual prompts of different forms. Instead of \"a photo of\", try \"a drawing of\", \"a picture of\", something else. Or \"a drawing of X, a type of Y\" as mentioned above. \n",
        "- To produce different results with the same prompt, try changing the seed. How do your results change?\n",
        "- Save any results you like. Since we seeded the random value and ran it fresh each time (\"Runtime->restart and run all\") these results should be replicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKvgZRpiuFeB"
      },
      "source": [
        "# References\n",
        "\n",
        "Other CLIP notebooks (all from [the Reddit r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais))\n",
        "- BigSLEEP (from [@advadnoun](https://twitter.com/advadnoun)): \n",
        "  - https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing\n",
        "- StyleGAN2 ADA with CLIP: \n",
        "  - https://colab.research.google.com/drive/1J8xyNRTNVnkNbQJnidcgSdDCHHKfGa8N?usp=sharing#scrollTo=I-YJmx89HLro\n",
        "- Aleph2Image (DALL-E encoder) with CLIP: \n",
        "  - https://colab.research.google.com/drive/1NGM9L8qP0gwl5z5GAuB_bd0wTNsxqclG\n",
        "- Lucent FFT and CLIP: \n",
        "  - https://colab.research.google.com/github/eps696/aphantasia/blob/master/Illustra.ipynb\n",
        "\n",
        "\n"
      ]
    }
  ]
}