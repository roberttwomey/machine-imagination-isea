{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_from_stored.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "o5YjaXywie10"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/machine-imagination-workshop/blob/main/generate_from_stored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# BigGAN + CLIP + CMA-ES: Interpolation\n",
        "\n",
        "This notebook takes the results from the BigGAN+CLIP+CMA-ES search for visual representations of texts, and generates interpolations that smoothly transition between these samples. Given a series of noise/class vectors (each a point in \"latent space\"), we will generate intermediate steps between each point, and save the result as a video smoothly transitioning between these points with a pause on each phrase/image. \n",
        "\n",
        "This video interpolation process is much faster than the text-to-image translation process.\n",
        "\n",
        "Please reach out with any thoughts, questions, or awesome results: [@roberttwomey](https://twitter.com/roberttwomey)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmKTmvBg7z5",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Setup software libraries (run once)\n",
        "#@markdown This cell installs the software libraries necessary to run our \n",
        "#@markdown text-to-image code on this Colab instance: CUDA, torch, torchvision.\n",
        "\n",
        "#@markdown Run this cell once - press the play button at top left.\n",
        "\n",
        "#@markdown Afterwards, restart the kernel. Select __Runtime -> Restart runtime__\n",
        "#@markdown from the top menu. Move on to Step 2 once you have restarted.\n",
        "\n",
        "#@markdown (this takes around 4-5 minutes to run)\n",
        "\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "# prints out what graphics card we have\n",
        "!nvidia-smi -L\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SDpkkK7cU1y",
        "cellView": "form"
      },
      "source": [
        "#@title 2. Install ML Models\n",
        "#@markdown Installs BigGAN â€” the image generator network. That is all we need\n",
        "#@markdown to create our latent walks. Everything else is already in colab.\n",
        "\n",
        "#@markdown (this takes around 1 minute to run)\n",
        "\n",
        "# BigGAN\n",
        "!pip install pytorch-pretrained-biggan\n",
        "\n",
        "from IPython.display import HTML, clear_output\n",
        "from PIL import Image\n",
        "from IPython.display import Image as JupImage\n",
        "import numpy as np\n",
        "import nltk\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "# from biggan\n",
        "import torch\n",
        "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
        "                                       save_as_images, convert_to_images) #, display_in_terminal)\n",
        "import logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "# do we need wordnet?\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# load biggan\n",
        "model = BigGAN.from_pretrained('biggan-deep-512')\n",
        "print(\"loaded bigGAN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Lddt4462o8",
        "cellView": "form"
      },
      "source": [
        "#@title 3. Upload your stored class and noise vectors\n",
        "\n",
        "#@markdown Click on \"Choose Files\" below, and select all of the _something_noise.txt_ \n",
        "#@markdown and _something_class.txt_ files from before. (For instance \"sunrise \n",
        "#@markdown through a window_1_noise.txt\", \"sunrise through a window_1_class.txt\")\n",
        "\n",
        "#@markdown Upload as many pairs of files as you would like. We will generate \n",
        "#@markdown your interpolation (\"latent walk\") between these images in latent  \n",
        "#@markdown space.\n",
        "\n",
        "#@markdown You should see your uploaded files in current directory if you click\n",
        "#@markdown on the folder icon at left. \n",
        "#@markdown If you have already uploaded files in this session, you can click \"cancel upload\".\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkAWJOI3---c"
      },
      "source": [
        "# set your prompts and order here (copy the text from above), but do not \n",
        "# include the \"_class.txt\" part or \"_noise.txt\" part. So just the stem of each\n",
        "# phrase. The order of phrases here determines the order of images in the output.\n",
        "# you can repeat images if you want.\n",
        "\n",
        "prompts = [\n",
        "    \"a sunrise through a window_1\",\n",
        "    \"a dog sitting on a couch_1\", \n",
        "    \"a cat in a refrigerator_255\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWzPLrBbdxW",
        "cellView": "form"
      },
      "source": [
        "#@title 4. Generate a latent walk!\n",
        "\n",
        "#@markdown This cell takes each of the images we generated before, and using\n",
        "#@markdown their locations in latent space (given by the noise and class vectors), \n",
        "#@markdown interpolates between them to create a smoothly flowing traversal \n",
        "#@markdown (\"walk\") through the space of possible images. \n",
        "\n",
        "#@markdown Set the following parameters to shape your output movie:\n",
        "#@markdown - fps is how many frames per second we want in the output video. \n",
        "#@markdown - num_steps is how many intermediate frames to generate between each \n",
        "#@markdown successive phrase/image\n",
        "#@markdown - len_hold is how many frames to pause/\"hold\" on each resultant image.\n",
        "\n",
        "# the movie\n",
        "fps = 30 #@param {type: 'number'}\n",
        "\n",
        "# the interpolation\n",
        "num_steps = 90 #@param {type:'number'}\n",
        "len_hold = 30 #@param {type: 'number'}\n",
        "\n",
        "truncation = 1.0\n",
        "\n",
        "interpbase = '/content/interpolation'\n",
        "!mkdir -p $interpbase\n",
        "moviefilename = 'interpolation_%s.mp4'\n",
        "\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import arccos\n",
        "from numpy import clip\n",
        "from numpy import dot\n",
        "from numpy import sin\n",
        "from numpy import linspace\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# from\n",
        "# https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/4\n",
        "\n",
        "# spherical linear interpolation (slerp)\n",
        "def slerp(val, low, high):\n",
        "    omega = arccos(clip(dot(low/norm(low), high/norm(high)), -1, 1))\n",
        "    so = sin(omega)\n",
        "    if so == 0:\n",
        "        # L'Hopital's rule/LERP\n",
        "        return (1.0-val) * low + val * high\n",
        "    return sin((1.0-val)*omega) / so * low + sin(val*omega) / so * high\n",
        " \n",
        "# uniform interpolation between two points in latent space\n",
        "def interpolate_points(p1, p2, n_steps=10):\n",
        "    # interpolate ratios between the points\n",
        "    ratios = np.linspace(0, 1, num=n_steps)\n",
        "    # linear interpolate vectors\n",
        "    vectors = list()\n",
        "    for ratio in ratios:\n",
        "        v = slerp(ratio, p1, p2)\n",
        "        vectors.append(v)\n",
        "    return np.asarray(vectors)\n",
        "\n",
        "def get_class_file(path, prompt):\n",
        "    # print(path+'%s*_class.txt'%prompt)\n",
        "    result = glob.glob(path+'%s*_class.txt'%prompt)\n",
        "    return(result)\n",
        "\n",
        "def get_noise_file(path, prompt):\n",
        "    # print(path+'%s*_noise.txt'%prompt)    \n",
        "    result = glob.glob(path+'%s*_noise.txt'%prompt)\n",
        "    return(result)\n",
        "\n",
        "class_filenames = [get_class_file('/content/', prompt)[0] for prompt in prompts]\n",
        "noise_filenames = [get_noise_file('/content/', prompt)[0] for prompt in prompts]\n",
        "\n",
        "# print(class_filenames, noise_filenames)\n",
        "\n",
        "class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
        "noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
        "\n",
        "# print(class_inputs, noise_inputs)\n",
        "\n",
        "count = 0\n",
        "\n",
        "# loop over inputs and generate interpolations\n",
        "for i in range(len(class_inputs)):\n",
        "\n",
        "    # generate interpolations\n",
        "    noises = interpolate_points(noise_inputs[i], \n",
        "                                noise_inputs[(i+1)%len(class_inputs)], \n",
        "                                num_steps)\n",
        "    classes = interpolate_points(class_inputs[i], \n",
        "                                 class_inputs[(i+1)%len(class_inputs)], \n",
        "                                 num_steps)\n",
        "\n",
        "    # generate images in batches\n",
        "    batch_size = 10 # 50\n",
        "    for j in range(0, num_steps, batch_size):\n",
        "\n",
        "        # clear_output()\n",
        "        print(i, j, count)\n",
        "        noise_vector = noises[j:j+batch_size]\n",
        "        class_vector = classes[j:j+batch_size]\n",
        "\n",
        "        # convert to tensors\n",
        "        noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
        "        class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
        "\n",
        "        # put everything on cuda (GPU)\n",
        "        noise_vector = noise_vector.to('cuda')\n",
        "        noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
        "        class_vector = class_vector.to('cuda')\n",
        "        class_vector = class_vector.softmax(dim=-1)\n",
        "        model.to('cuda')\n",
        "\n",
        "        # generate images\n",
        "        with torch.no_grad():\n",
        "            #print(noise_vector.shape)\n",
        "            #print(class_vector.shape)\n",
        "            output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "        # If you have a GPU put back on CPU\n",
        "        output = output.to('cpu')\n",
        "\n",
        "        imgs = convert_to_images(output)\n",
        "\n",
        "        # repeat first image\n",
        "        \n",
        "        if j == 0:\n",
        "            for k in range(len_hold):\n",
        "                imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
        "                count = count + 1\n",
        "                \n",
        "        for img in imgs: \n",
        "            img.save(interpbase+\"/output_%05d.png\" % count)\n",
        "            count = count + 1\n",
        "\n",
        "# generate mp4\n",
        "out = moviefilename%fps\n",
        "with open('list.txt','w') as f:\n",
        "  for i in range(count):\n",
        "    # print('file %s/output_%05d.png\\n'%(interpbase, i))\n",
        "    f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
        "!ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y -loglevel error -stats\n",
        "# !echo ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
        "        \n",
        "\n",
        "# display movie in notebook\n",
        "with open(moviefilename%fps, 'rb') as f:\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "display(HTML(\"\"\"\n",
        "  <video controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\"\"\" % data_url))\n",
        "\n",
        "# play \"ding\" and download movie\n",
        "from google.colab import files, output\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "files.download(moviefilename%fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5YjaXywie10"
      },
      "source": [
        "# Explanation\n",
        "\n",
        "To learn more about BigGAN, latent space, and interpolations between noise/class vectors, see [this colab notebook](https://colab.research.google.com/github/roberttwomey/machine-imagination-workshop/blob/main/BigGAN_handson.ipynb).\n",
        "\n",
        "It walks through examples of sampling from different categories (`\"933) cheeseburger\"` for example), and lets you play with the role the noise and truncation play in generation. \n",
        "\n",
        "It also shows you how to interpolate between two samples with a fixed number of steps. This is what we are doing here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKvgZRpiuFeB"
      },
      "source": [
        "# References\n",
        "- pytorch pretrained BigGAN from huggingface https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
        "- Hands-on with BigGAN from Google (2018): [colab notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)"
      ]
    }
  ]
}